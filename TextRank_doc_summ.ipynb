{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from BeautifulSoup import BeautifulSoup\n",
    "import nltk as nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import webbrowser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('/home/pragati/holdoutHalf/06_3.xml', 'r')\n",
    "text_full = f.read()\n",
    "soup = BeautifulSoup(text_full) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"/home/pragati/Git_Pragati/RTC-Document_Summarization/imp_dict.dat\",'r') as p:\n",
    "    imp_dict = pickle.load(p)\n",
    "imp_pick=imp_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glob_text = soup('sentences')[0].text.split('\\n')\n",
    "curated_glob = [sent.lstrip('0123456789.-\" ') for sent in glob_text if len(sent) > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = []\n",
    "i_range = len(curated_glob)\n",
    "copy_curated = copy.copy(curated_glob)\n",
    "for i in range(i_range):\n",
    "    tokens = [w.lower() for w in copy_curated[i].split()]\n",
    "    punc_rem = [re.sub('[^A-Za-z]+', '', word) for word in tokens]\n",
    "    #lemmatized = [lemm.lemmatize(w) for w in punc_rem]\n",
    "    filt_stop = [word for word in punc_rem if word not in stopwords.words('english')]\n",
    "    if len(filt_stop) < 10:\n",
    "        to_rem = copy_curated[i]\n",
    "        curated_glob.remove(to_rem)\n",
    "    else:\n",
    "        text.append(filt_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def intersection_comp(s_1, s_2):\n",
    "    s1 = set(s_1)\n",
    "    s2 = set(s_2)\n",
    "    if (len(s1) + len(s2)) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        # We normalize the result by the average number of words\n",
    "        return len(s1.intersection(s2)) / float(((len(s1) + len(s2)) / 2))\n",
    "def intersection_corpus(s_1):\n",
    "    s1 = set(s_1)\n",
    "    s2 = set(imp_pick)\n",
    "    if len(s1) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(s1.intersection(s2)) / float(len(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = len(text)\n",
    "values = np.zeros((n,n+1), dtype = np.double)\n",
    "for i in range(0, n):\n",
    "    for j in range(0, n):\n",
    "        values[i][j] = intersection_comp(text[i], text[j])\n",
    "    values[i][n] = intersection_corpus(text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_list = []\n",
    "for i in range(0, n):\n",
    "    score = 0\n",
    "    for j in range(0, n):\n",
    "        if i == j:\n",
    "            continue\n",
    "        score += values[i][j]\n",
    "    score_list.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys =[]\n",
    "text_dict = {}\n",
    "norm_score = [float(i)/max(score_list) for i in score_list]\n",
    "for i in range(0,n):\n",
    "    final = (0.3* norm_score[i]) + (0.7 * values[i][n])\n",
    "    #final = (values[i][n])\n",
    "    keys.append(\" \".join(text[i]))\n",
    "    text_dict[\" \".join(text[i])] = final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set_summary =[]\n",
    "set_tokensent=[]\n",
    "thresh = np.sort(text_dict.values())[::-1][:10][9]\n",
    "for i in range(n):\n",
    "    if text_dict[keys[i]] >= thresh:\n",
    "        to_append1 = str(re.sub('[^A-Za-z0-9]+', ' ', curated_glob[i].lstrip('0123456789.- ')))\n",
    "        set_summary.append(to_append1)\n",
    "        set_tokensent.append(\" \".join(text[i]))\n",
    "#summary = list(set(set_summary))\n",
    "#summary_token = list(set(set_tokensent))\n",
    "#for j in range(len(set_summary)):\n",
    "#    print set_summary[j], \".\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_key = defaultdict(list)\n",
    "num_iter = len(set_tokensent)\n",
    "for i in range(num_iter):\n",
    "    sel_word = 'other'\n",
    "    max_rank = 0\n",
    "    for word in set_tokensent[i].split():\n",
    "        if word in imp_pick:\n",
    "            rank = imp_dict[word]\n",
    "            if rank>max_rank:\n",
    "                max_rank = rank\n",
    "                sel_word = word\n",
    "    dict_key[sel_word].append(set_summary[i])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write(data):\n",
    "    print('Creating new json file') \n",
    "    name = '/home/pragati/HACK101/flare.json'  # Name of text file coerced with +.txt\n",
    "    with open(name, 'w') as outfile:\n",
    "        json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createJson(summary):\n",
    "    jsonObj = {}\n",
    "    jsonObj[\"name\"] = \"Document\"\n",
    "    children = []\n",
    "    for key,value in summary.iteritems():\n",
    "        catchsent=[]\n",
    "        for sentence in value:\n",
    "            catchsent.append({\"name\" : sentence,\"size\" : 3219})\n",
    "        children.append({\"name\" : key,\"children\" : catchsent})\n",
    "    jsonObj[\"children\"]=children\n",
    "    return jsonObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsonObj=createJson(dict_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new json file\n"
     ]
    }
   ],
   "source": [
    "write(jsonObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = '/home/pragati/HACK101/index.html'\n",
    "webbrowser.open_new_tab(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
